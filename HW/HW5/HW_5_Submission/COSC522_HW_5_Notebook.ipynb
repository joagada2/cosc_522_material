{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87fe21d7-01e5-439c-bf9c-cf11bd23eeae",
   "metadata": {},
   "source": [
    "# COSC522 HOMEWORK 5 PYTHON IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397798aa-6cc4-40cc-afc6-7f92c791dd29",
   "metadata": {},
   "source": [
    "# PROBLEM 1 (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52895f1f-a3ae-4f3d-9326-c6d3292a1425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naïve Bayes Fusion Lookup Table:\n",
      "   L1  L2  Fused Label\n",
      "0   1   1            1\n",
      "1   1   2            2\n",
      "2   1   3            1\n",
      "3   2   1            1\n",
      "4   2   2            2\n",
      "5   2   3            3\n",
      "6   3   1            1\n",
      "7   3   2            3\n",
      "8   3   3            3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Confusion matrices for L1 and L2\n",
    "conf_matrix_L1 = np.array([\n",
    "    [20, 5, 5],\n",
    "    [3, 24, 3],\n",
    "    [0, 9, 21]\n",
    "])\n",
    "\n",
    "conf_matrix_L2 = np.array([\n",
    "    [25, 2, 3],\n",
    "    [3, 22, 5],\n",
    "    [5, 6, 19]\n",
    "])\n",
    "\n",
    "# Class priors (equal distribution as each class has 30 samples)\n",
    "num_samples_per_class = 30\n",
    "total_samples = 3 * num_samples_per_class\n",
    "priors = np.array([num_samples_per_class / total_samples] * 3)  # [P(w1), P(w2), P(w3)]\n",
    "\n",
    "# Normalize confusion matrices to compute likelihoods P(L1 | w) and P(L2 | w)\n",
    "likelihoods_L1 = conf_matrix_L1 / np.sum(conf_matrix_L1, axis=1, keepdims=True)\n",
    "likelihoods_L2 = conf_matrix_L2 / np.sum(conf_matrix_L2, axis=1, keepdims=True)\n",
    "\n",
    "# Generate the lookup table for Naïve Bayes fusion\n",
    "lookup_table = []\n",
    "for l1 in range(3):  # L1 predictions (0-based index)\n",
    "    for l2 in range(3):  # L2 predictions (0-based index)\n",
    "        posteriors = []\n",
    "        for w in range(3):  # True class w\n",
    "            posterior = likelihoods_L1[w, l1] * likelihoods_L2[w, l2] * priors[w]\n",
    "            posteriors.append(posterior)\n",
    "        # Normalize posteriors and choose the class with max posterior probability\n",
    "        posteriors = np.array(posteriors) / np.sum(posteriors)\n",
    "        fused_label = np.argmax(posteriors) + 1  # Convert to 1-based indexing\n",
    "        lookup_table.append([l1 + 1, l2 + 1, fused_label])  # Convert indices to 1-based\n",
    "\n",
    "# Convert lookup table to DataFrame for better readability\n",
    "lookup_df = pd.DataFrame(lookup_table, columns=[\"L1\", \"L2\", \"Fused Label\"])\n",
    "print(\"Naïve Bayes Fusion Lookup Table:\")\n",
    "print(lookup_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6177b8-998b-41e1-9ee8-7abc4bde4aa0",
   "metadata": {},
   "source": [
    "# PROBLEM 1 (B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdda9984-1996-4c0c-ad20-bd0631d4d803",
   "metadata": {},
   "source": [
    "### It is not possible because BKS Table requires joint distribution of the classifiers outputs and true class labels from training data. The confusion matrices do not provide these actual class labels and joint distribution information.\n",
    "\n",
    "### Difference Between Naive Bayes and BKS is that:\n",
    "\n",
    " - Naive Bayes assumes classifier independence and combine probabilities analytically while\n",
    " - BKS directly uses observed joint distribution of classifier output without independence assumption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77560df-ea18-440e-afaf-74aeda3bf88e",
   "metadata": {},
   "source": [
    "# PROBLEM 1 (C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0696ec1-1ecb-436a-997d-8d0b511abfef",
   "metadata": {},
   "source": [
    "### Yes, given a BKS table, I can derive the Naive Bayes look-up table first by assuming or calculating the prior and calculating the marginal probabilities by summing over the observed counts in the BKS table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fc3a76-ad19-4ae4-a009-f0e4003b76c9",
   "metadata": {},
   "source": [
    "# PROBLEM 2 (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c082457-e684-497a-b1ac-3c30c18e76c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Predictions (L1 Boost): [1160.192  1199.872  1279.872  1571.7888 1878.9888]\n",
      "Mean Squared Error (L1 Boost): 5895.260768362879\n",
      "Final Predictions (L2 Boost): [1180.1207 1196.6356 1279.7759 1447.3866 1954.7437]\n",
      "Mean Squared Error (L2 Boost): 494.2359678119421\n",
      "Final Predictions (Simple Linear Regression): [1032. 1225. 1418. 1611. 1804.]\n",
      "Mean Squared Error (Simple Linear Regression): 20077.999999999964\n"
     ]
    }
   ],
   "source": [
    "# Dataset: Square Footage (x) and Rent (y)\n",
    "x = np.array([750, 800, 850, 900, 950]).reshape(-1, 1)  # Square footage\n",
    "y = np.array([1160, 1200, 1280, 1450, 2000])  # Rent prices\n",
    "\n",
    "# L1 Boost using XGBoost\n",
    "def xgboost_l1_boost(x, y, num_rounds=5):\n",
    "    \"\"\"\n",
    "    Implements L1 Boost using XGBoost.\n",
    "    \"\"\"\n",
    "    dtrain = xgb.DMatrix(x, label=y)\n",
    "    params = {\n",
    "        'objective': 'reg:absoluteerror',  # L1 loss\n",
    "        'learning_rate': 0.8,\n",
    "        'max_depth': 2,\n",
    "        'verbosity': 0\n",
    "    }\n",
    "    model = xgb.train(params, dtrain, num_boost_round=num_rounds)\n",
    "    preds = model.predict(dtrain)\n",
    "    mse = mean_squared_error(y, preds)\n",
    "    return preds, mse\n",
    "\n",
    "# L2 Boost using XGBoost\n",
    "def xgboost_l2_boost(x, y, num_rounds=5):\n",
    "    \"\"\"\n",
    "    Implements L2 Boost using XGBoost.\n",
    "    \"\"\"\n",
    "    dtrain = xgb.DMatrix(x, label=y)\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror',  # L2 loss\n",
    "        'learning_rate': 0.8,\n",
    "        'max_depth': 2,\n",
    "        'verbosity': 0\n",
    "    }\n",
    "    model = xgb.train(params, dtrain, num_boost_round=num_rounds)\n",
    "    preds = model.predict(dtrain)\n",
    "    mse = mean_squared_error(y, preds)\n",
    "    return preds, mse\n",
    "\n",
    "# Simple Linear Regression\n",
    "def simple_linear_regression(x, y):\n",
    "    \"\"\"\n",
    "    Simple Linear Regression implementation using scikit-learn.\n",
    "    \"\"\"\n",
    "    model = LinearRegression()\n",
    "    model.fit(x, y)\n",
    "    preds = model.predict(x)\n",
    "    mse = mean_squared_error(y, preds)\n",
    "    return preds, mse\n",
    "\n",
    "# Run L1 Boost\n",
    "l1_preds, l1_mse = xgboost_l1_boost(x, y, num_rounds=5)\n",
    "print(\"Final Predictions (L1 Boost):\", l1_preds)\n",
    "print(\"Mean Squared Error (L1 Boost):\", l1_mse)\n",
    "\n",
    "# Run L2 Boost\n",
    "l2_preds, l2_mse = xgboost_l2_boost(x, y, num_rounds=5)\n",
    "print(\"Final Predictions (L2 Boost):\", l2_preds)\n",
    "print(\"Mean Squared Error (L2 Boost):\", l2_mse)\n",
    "\n",
    "# Run Simple Linear Regression\n",
    "linear_preds, linear_mse = simple_linear_regression(x, y)\n",
    "print(\"Final Predictions (Simple Linear Regression):\", linear_preds)\n",
    "print(\"Mean Squared Error (Simple Linear Regression):\", linear_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d25ef25-b637-4499-bbb3-c41b178df0b8",
   "metadata": {},
   "source": [
    "### COMPARISON: MSE for L1 Boost and L2 Boost are both lower than MSE for Simple Linear Regression hence L1 Boost and L2 Boost perform better then Simple Linear Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fda18b-84a0-40df-801b-e94f7ab4da13",
   "metadata": {},
   "source": [
    "# PROBLEM 2(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a736339-bffe-4e11-af07-5ded6083e985",
   "metadata": {},
   "source": [
    "It is called gradient boosting because it uses the principle of gradient descent to optimize a loss function during the boosting process. AT each iteration, of the algorithm, the model learns by minimizing either the mean absolute error (for L1 Boost) or mean squared error (for L2Boost).\n",
    "\n",
    "Boosting is an ensemble technique tha combines multiple weak learniners to ctreate a strong learner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71218d1c-2454-4f15-9573-7ad79d257dbc",
   "metadata": {},
   "source": [
    "# PROBLEM 2 (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4cf8ea46-3295-4ff5-b92d-46f5506ccff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Predictions (L2 Boost): [0.10891263 0.26561332 0.60602224 0.8729689  0.9393056 ]\n",
      "Mean Squared Error (L2 Boost): 0.0005024970504785359\n",
      "Final Predictions (Simple Linear Regression): [0.074 0.315 0.556 0.797 1.038]\n",
      "Mean Squared Error (Simple Linear Regression): 0.003182000000000003\n"
     ]
    }
   ],
   "source": [
    "# Dataset: Square Footage (x) and Rent (y)\n",
    "x = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)  # Square footage\n",
    "y = np.array([0.07, 0.26, 0.61, 0.87, 0.97])  # Rent prices\n",
    "\n",
    "# L2 Boost using XGBoost\n",
    "def xgboost_l2_boost(x, y, num_rounds=5):\n",
    "    \"\"\"\n",
    "    Implements L2 Boost using XGBoost.\n",
    "    \"\"\"\n",
    "    dtrain = xgb.DMatrix(x, label=y)\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror',  # L2 loss\n",
    "        'learning_rate': 0.8,\n",
    "        'max_depth': 2,\n",
    "        'verbosity': 0\n",
    "    }\n",
    "    model = xgb.train(params, dtrain, num_boost_round=num_rounds)\n",
    "    preds = model.predict(dtrain)\n",
    "    mse = mean_squared_error(y, preds)\n",
    "    return preds, mse\n",
    "\n",
    "# Simple Linear Regression\n",
    "def simple_linear_regression(x, y):\n",
    "    \"\"\"\n",
    "    Simple Linear Regression implementation using scikit-learn.\n",
    "    \"\"\"\n",
    "    model = LinearRegression()\n",
    "    model.fit(x, y)\n",
    "    preds = model.predict(x)\n",
    "    mse = mean_squared_error(y, preds)\n",
    "    return preds, mse\n",
    "\n",
    "# Run L2 Boost\n",
    "l2_preds, l2_mse = xgboost_l2_boost(x, y, num_rounds=5)\n",
    "print(\"Final Predictions (L2 Boost):\", l2_preds)\n",
    "print(\"Mean Squared Error (L2 Boost):\", l2_mse)\n",
    "\n",
    "# Run Simple Linear Regression\n",
    "linear_preds, linear_mse = simple_linear_regression(x, y)\n",
    "print(\"Final Predictions (Simple Linear Regression):\", linear_preds)\n",
    "print(\"Mean Squared Error (Simple Linear Regression):\", linear_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc2c832-111d-4d2a-b4ab-67b8d0753872",
   "metadata": {},
   "source": [
    "# PROBLEM 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee493a15-21e8-407f-ae37-f151b1ce45f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini Impurity for Option 1: 0.1333\n",
      "Gini Impurity for Option 2: 0.1000\n",
      "Better Query: Option 2\n"
     ]
    }
   ],
   "source": [
    "def gini_impurity(p):\n",
    "    \"\"\"\n",
    "    Calculate the Gini impurity for a given probability distribution.\n",
    "    \"\"\"\n",
    "    return 1 - sum([pi**2 for pi in p])\n",
    "\n",
    "def weighted_gini(left_counts, right_counts):\n",
    "    \"\"\"\n",
    "    Calculate the weighted Gini impurity for a split.\n",
    "    \"\"\"\n",
    "    total = sum(left_counts) + sum(right_counts)\n",
    "    \n",
    "    # Gini for the left node\n",
    "    left_total = sum(left_counts)\n",
    "    left_gini = gini_impurity([count / left_total for count in left_counts]) if left_total > 0 else 0\n",
    "\n",
    "    # Gini for the right node\n",
    "    right_total = sum(right_counts)\n",
    "    right_gini = gini_impurity([count / right_total for count in right_counts]) if right_total > 0 else 0\n",
    "\n",
    "    # Weighted Gini\n",
    "    weighted = (left_total / total) * left_gini + (right_total / total) * right_gini\n",
    "    return weighted\n",
    "\n",
    "# 3 (a) \n",
    "# Option 1: Split information\n",
    "left_counts_option_1 = [70, 0]  # [Class 1, Class 2]\n",
    "right_counts_option_1 = [20, 10]  # [Class 1, Class 2]\n",
    "\n",
    "# 3 (b)\n",
    "# Option 2: Split information\n",
    "left_counts_option_2 = [80, 0]  # [Class 1, Class 2]\n",
    "right_counts_option_2 = [10, 10]  # [Class 1, Class 2]\n",
    "\n",
    "# Calculate Gini impurity for both options\n",
    "gini_option_1 = weighted_gini(left_counts_option_1, right_counts_option_1)\n",
    "gini_option_2 = weighted_gini(left_counts_option_2, right_counts_option_2)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Gini Impurity for Option 1: {gini_option_1:.4f}\")\n",
    "print(f\"Gini Impurity for Option 2: {gini_option_2:.4f}\")\n",
    "\n",
    "# Determine the better query based on Gini impurity\n",
    "better_option = \"Option 1\" if gini_option_1 < gini_option_2 else \"Option 2\"\n",
    "print(f\"Better Query: {better_option}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f5db74-7db2-4563-b2df-95ea8975f24c",
   "metadata": {},
   "source": [
    "# BONUS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2e6ad0-eea1-483c-954d-c1f4e77960fe",
   "metadata": {},
   "source": [
    "# Modifying kNN to Incorporate Prior Probabilities\n",
    "\n",
    "In standard k-Nearest Neighbors (kNN), the prior probability for a class is fixed at $n_k / n$, where $n_k$ is the number of training samples in class $k$ and $n$ is the total number of training samples. To allow the incorporation of different prior probabilities, the following potential modifications can be made:\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Weighted Voting Based on Prior Probabilities\n",
    "In standard kNN, each neighbor contributes equally to the classification vote. To incorporate prior probabilities, the voting process can be modified so that votes from neighbors belonging to class $c$ are weighted by the desired prior probability $P(c)$.\n",
    "\n",
    "The score for each class can be calculated as:\n",
    "$$\n",
    "\\text{Score}(c) = \\sum_{i \\in \\text{neighbors}(c)} \\frac{P(c)}{\\text{distance}(i, \\text{query})^\\alpha},\n",
    "$$\n",
    "where:\n",
    "- $P(c)$ is the desired prior for class $c$,\n",
    "- $\\text{distance}(i, \\text{query})$ is the distance between neighbor $i$ and the query point,\n",
    "- $\\alpha$ controls the influence of distance on weighting (e.g., $\\alpha = 1$).\n",
    "\n",
    "The query point is assigned the class with the highest score.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Resampling Training Data According to Desired Priors\n",
    "To simulate different priors, the training dataset can be adjusted by oversampling or undersampling each class. \n",
    "\n",
    "For a class $c$ with desired prior $P(c)$:\n",
    "1. Compute the desired number of samples for class $c$: \n",
    "   $$\n",
    "   n_c^{\\text{desired}} = n \\cdot P(c),\n",
    "   $$\n",
    "   where $n$ is the total number of samples in the training set.\n",
    "2. Resample the training data for class $c$ to match $n_c^{\\text{desired}}$.\n",
    "\n",
    "This modification alters the class distribution in the training dataset to reflect the desired priors.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Bayesian Adjustment of Posteriors\n",
    "Bayes' Theorem can be used to adjust the decision-making process to incorporate prior probabilities:\n",
    "$$\n",
    "P(c \\mid \\text{neighbors}) \\propto P(c) \\cdot P(\\text{neighbors} \\mid c),\n",
    "$$\n",
    "where:\n",
    "- $P(c)$ is the prior probability for class $c$,\n",
    "- $P(\\text{neighbors} \\mid c)$ is the likelihood, proportional to the number of neighbors belonging to class $c$.\n",
    "\n",
    "### Steps:\n",
    "1. Count the neighbors belonging to each class.\n",
    "2. Multiply these counts by the respective class priors $P(c)$.\n",
    "3. Normalize the adjusted counts to get posterior probabilities:\n",
    "   $$\n",
    "   P(c \\mid \\text{neighbors}) = \\frac{P(c) \\cdot P(\\text{neighbors} \\mid c)}{\\sum_{k} P(k) \\cdot P(\\text{neighbors} \\mid k)}.\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Adjusting Decision Thresholds\n",
    "If the output of kNN includes probabilities for each class, the decision threshold can be modified to align with the desired priors. For binary classification:\n",
    "- The default threshold is $0.5$.\n",
    "- To incorporate a prior $P(c=1)$, adjust the threshold to:\n",
    "  $$\n",
    "  t = P(c=1).\n",
    "  $$\n",
    "\n",
    "This approach is simple and does not require modifying the core kNN algorithm.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. kNN with Weighted Distance Metrics\n",
    "The distance metric used in kNN can be adjusted to reflect class priors:\n",
    "$$\n",
    "d'(i, \\text{query}) = d(i, \\text{query}) \\cdot \\frac{1}{P(c(i))},\n",
    "$$\n",
    "where $c(i)$ is the class of neighbor $i$, and $P(c(i))$ is the prior probability for class $c(i)$. \n",
    "\n",
    "This adjustment prioritizes neighbors from underrepresented classes and penalizes neighbors from overrepresented classes.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "1. **Weighted Voting** and **Bayesian Adjustment** allow priors to be directly incorporated into kNN's decision-making process.\n",
    "2. **Resampling Training Data** modifies the dataset to reflect the desired priors.\n",
    "3. **Threshold Adjustment** is simple and effective for tasks requiring posterior probabilities.\n",
    "4. **Weighted Distance Metrics** prioritize neighbor selection based on priors.\n",
    "\n",
    "The choice of method depends on the specific requirements of the task and the ease of implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb1455b-5d09-4eb9-9a3b-8f943a113a8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
